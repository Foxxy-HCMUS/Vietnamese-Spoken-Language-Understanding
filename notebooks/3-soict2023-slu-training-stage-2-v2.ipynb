{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset, load_from_disk\nimport pandas as pd\nimport json\npd.set_option('display.max_colwidth', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-31T11:41:43.818197Z","iopub.execute_input":"2023-08-31T11:41:43.818570Z","iopub.status.idle":"2023-08-31T11:41:44.769611Z","shell.execute_reply.started":"2023-08-31T11:41:43.818539Z","shell.execute_reply":"2023-08-31T11:41:44.768632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras_preprocessing","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:41:44.773268Z","iopub.execute_input":"2023-08-31T11:41:44.773859Z","iopub.status.idle":"2023-08-31T11:42:00.132380Z","shell.execute_reply.started":"2023-08-31T11:41:44.773831Z","shell.execute_reply":"2023-08-31T11:42:00.131084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download wordsegment package","metadata":{}},{"cell_type":"code","source":"!pip install vncorenlp\n!mkdir -p vncorenlp/models/wordsegmenter\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n!mv vi-vocab vncorenlp/models/wordsegmenter/\n!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:00.137982Z","iopub.execute_input":"2023-08-31T11:42:00.140464Z","iopub.status.idle":"2023-08-31T11:42:24.767157Z","shell.execute_reply.started":"2023-08-31T11:42:00.140414Z","shell.execute_reply":"2023-08-31T11:42:24.765834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load datasets for stage 2","metadata":{}},{"cell_type":"markdown","source":"## Load train data","metadata":{}},{"cell_type":"code","source":"lines = []\nwith open(\"/kaggle/input/soict2023-slu/SLU/train_20230909.jsonl\") as f:\n    for line in f.readlines():\n        lines.append(json.loads(line))\ntrain_stage2 = pd.DataFrame(lines)\ntrain_stage2.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:02:07.482992Z","iopub.execute_input":"2023-08-31T14:02:07.483449Z","iopub.status.idle":"2023-08-31T14:02:07.681078Z","shell.execute_reply.started":"2023-08-31T14:02:07.483412Z","shell.execute_reply":"2023-08-31T14:02:07.680013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stage2[\"intent\"].unique()","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:02:10.605358Z","iopub.execute_input":"2023-08-31T14:02:10.605733Z","iopub.status.idle":"2023-08-31T14:02:10.615471Z","shell.execute_reply.started":"2023-08-31T14:02:10.605702Z","shell.execute_reply":"2023-08-31T14:02:10.614409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stage2[\"entities\"].apply(lambda x: \n                               [list(i.values())[0] for i in x]).explode().unique()","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:02:14.306397Z","iopub.execute_input":"2023-08-31T14:02:14.306789Z","iopub.status.idle":"2023-08-31T14:02:14.339705Z","shell.execute_reply.started":"2023-08-31T14:02:14.306756Z","shell.execute_reply":"2023-08-31T14:02:14.338738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word segment","metadata":{}},{"cell_type":"code","source":"from vncorenlp import VnCoreNLP\nrdrsegmenter = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n\ntext = 'Tôn Ngộ Không phò Đường Tăng đi thỉnh kinh tại Tây Trúc'\nwords = rdrsegmenter.tokenize(text)[0]\nprint('text_masked_tok: \\n', words)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:25.121883Z","iopub.execute_input":"2023-08-31T11:42:25.124208Z","iopub.status.idle":"2023-08-31T11:42:30.597925Z","shell.execute_reply.started":"2023-08-31T11:42:25.124172Z","shell.execute_reply":"2023-08-31T11:42:30.596750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pandarallel\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:30.599690Z","iopub.execute_input":"2023-08-31T11:42:30.600059Z","iopub.status.idle":"2023-08-31T11:42:45.071215Z","shell.execute_reply.started":"2023-08-31T11:42:30.600004Z","shell.execute_reply":"2023-08-31T11:42:45.069771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import re\n# chars_to_ignore_regex = '[\\`\\'\\,\\?\\.\\!\\-\\;\\:\\/\"]'\n\n# def preprocessing(x):\n#     x = x.strip()\n#     words = []\n#     for word in x.split():\n#         word = word.strip()\n#         words.append(word)\n#     x = \" \".join(words) \n#     x = re.sub(chars_to_ignore_regex, '', x)\n#     x = rdrsegmenter.tokenize(x.lower())[0]\n#     return x\n\n# train_stage2[\"words\"] = train_stage2[\"sentence\"].parallel_apply(preprocessing)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:45.073267Z","iopub.execute_input":"2023-08-31T11:42:45.073655Z","iopub.status.idle":"2023-08-31T11:42:45.079725Z","shell.execute_reply.started":"2023-08-31T11:42:45.073618Z","shell.execute_reply":"2023-08-31T11:42:45.078585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stage2[train_stage2[\"sentence\"].str.contains(\"/\")]","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:45.086259Z","iopub.execute_input":"2023-08-31T11:42:45.086646Z","iopub.status.idle":"2023-08-31T11:42:45.115106Z","shell.execute_reply.started":"2023-08-31T11:42:45.086611Z","shell.execute_reply":"2023-08-31T11:42:45.113966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Replace entities in the sentence","metadata":{}},{"cell_type":"code","source":"import re\nchars_to_ignore_regex = '[\\`\\'\\?\\.\\!\\-\\;\\/\"]'\ndef remove_special_characters(x):\n    x = x.lower().strip()\n    x = re.sub(chars_to_ignore_regex, '', x)\n    x = re.sub(\",\", \" \", x)\n    return x\n\ndef replace_entities(x):\n    x = remove_special_characters(x)\n    replacement = \"_\"\n    x = re.sub(r'(\\S*)\\[([^:\\]]+):\\s*([^:\\]]+)\\](\\S*)', \n               lambda match: match.group(1) + \" [\" + match.group(2) + \" : \" + match.group(3) + \"] \" + match.group(4), x)\n#     x = re.sub(r'\\[([^:\\]]+):\\s*([^:\\]]+)\\]', \n#             lambda match: match.group(2).strip().replace(\" \", replacement), x)\n    return x\n\ndef word_segment(x):\n    x = rdrsegmenter.tokenize(x)[0]\n    return x\n\ndef preprocessing(x):\n    x = replace_entities(x)\n#     x = word_segment(x)\n    return x\n\n# train_stage2[\"words\"] = train_stage2[\"sentence\"].parallel_apply(replace_entities).str.split()","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:45.117087Z","iopub.execute_input":"2023-08-31T11:42:45.117507Z","iopub.status.idle":"2023-08-31T11:42:45.377438Z","shell.execute_reply.started":"2023-08-31T11:42:45.117469Z","shell.execute_reply":"2023-08-31T11:42:45.376291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make words (tokenize the sentence)","metadata":{}},{"cell_type":"code","source":"import re\ndef make_words(df):\n    labels = []\n    # Pattern to extract the annotations [ label : value ]\n    pattern = r\"\\[([^\\]]+)\\]\"\n\n    sentence_standard = remove_special_characters(df[\"sentence_annotation\"])\n    words = re.split(r\"(\\[[^\\]]+\\])\", sentence_standard)\n    results = []\n    for word in words:\n        match = re.findall(pattern, word)\n        if match:\n            parts = match[0].split(\":\")\n            value = parts[1].strip()\n            results.extend(value.split())\n        else:\n            results.extend(word.strip().split())\n    return results \ntrain_stage2[\"words\"] = train_stage2.parallel_apply(make_words, axis=1)\ntrain_stage2[\"words\"]","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:05:15.385631Z","iopub.execute_input":"2023-08-31T14:05:15.386361Z","iopub.status.idle":"2023-08-31T14:05:16.092231Z","shell.execute_reply.started":"2023-08-31T14:05:15.386313Z","shell.execute_reply":"2023-08-31T14:05:16.090977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make tags for the sentences","metadata":{}},{"cell_type":"code","source":"train_stage2[\"tags\"] = train_stage2[\"entities\"].parallel_apply(lambda x: \n        [{list(i.values())[1].lower().strip().replace(\" \", \"_\"): list(i.values())[0]} for i in x])","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:05:19.606863Z","iopub.execute_input":"2023-08-31T14:05:19.607294Z","iopub.status.idle":"2023-08-31T14:05:19.945642Z","shell.execute_reply.started":"2023-08-31T14:05:19.607257Z","shell.execute_reply":"2023-08-31T14:05:19.944332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stage2[\"tags\"]","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:05:24.126187Z","iopub.execute_input":"2023-08-31T14:05:24.126608Z","iopub.status.idle":"2023-08-31T14:05:24.144357Z","shell.execute_reply.started":"2023-08-31T14:05:24.126567Z","shell.execute_reply":"2023-08-31T14:05:24.143233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stage2.head(1)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:05:26.377743Z","iopub.execute_input":"2023-08-31T14:05:26.378147Z","iopub.status.idle":"2023-08-31T14:05:26.398173Z","shell.execute_reply.started":"2023-08-31T14:05:26.378113Z","shell.execute_reply":"2023-08-31T14:05:26.396914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_mapping = {i:i for i in train_stage2[\"entities\"].apply(lambda x: \n                               [list(i.values())[0] for i in x]).explode().unique()}\n\nimport re\ndef make_tags(df):\n    labels = []\n    # Pattern to extract the annotations [ label : value ]\n    pattern = r\"\\[([^\\]]+)\\]\"\n\n    sentence_standard = remove_special_characters(df[\"sentence_annotation\"])\n    sentence_standard = re.sub(r'(\\S*)\\[([^:\\]]+):\\s*([^:\\]]+)\\](\\S*)', \n            lambda match: match.group(1) + \" [\" + match.group(2) + \" : \" + match.group(3) + \"] \" + match.group(4), sentence_standard)\n    # Extract annotations from the sentence\n    annotations = [tag.strip() for tag in re.findall(pattern, \n                                    sentence_standard)]\n\n    words = re.split(r\"(\\[[^\\]]+\\])\", sentence_standard)\n    for word in words:\n        match = re.findall(pattern, word)\n        if match:\n            parts = match[0].split(\":\")\n            label = parts[0].strip()\n            value = parts[1].strip()\n            mapped_label = label_mapping.get(label, \"O\")\n            labels.extend([mapped_label] * len(value.split()))\n        else:\n            labels.extend([\"O\"] * len(word.split()))\n            \n    return labels\n    \ntrain_stage2[\"tags\"] = train_stage2.parallel_apply(make_tags, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:05:30.206816Z","iopub.execute_input":"2023-08-31T14:05:30.207996Z","iopub.status.idle":"2023-08-31T14:05:31.555136Z","shell.execute_reply.started":"2023-08-31T14:05:30.207945Z","shell.execute_reply":"2023-08-31T14:05:31.553658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stage2[[\"sentence_annotation\", \"words\", \"tags\"]].sample(3)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:05:33.510179Z","iopub.execute_input":"2023-08-31T14:05:33.510577Z","iopub.status.idle":"2023-08-31T14:05:33.543813Z","shell.execute_reply.started":"2023-08-31T14:05:33.510539Z","shell.execute_reply":"2023-08-31T14:05:33.542690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wrong_anno = train_stage2[train_stage2[\"words\"].apply(len) != train_stage2[\"tags\"].apply(len)]\nwrong_anno","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:06:51.522256Z","iopub.execute_input":"2023-08-31T14:06:51.522702Z","iopub.status.idle":"2023-08-31T14:06:51.545493Z","shell.execute_reply.started":"2023-08-31T14:06:51.522656Z","shell.execute_reply":"2023-08-31T14:06:51.544164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vì đề bài không cho relabel lại các annotation bị sai, do đó ta sẽ giữ nguyên mà không fix các annotation này","metadata":{}},{"cell_type":"code","source":"wrong_anno.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-31T14:05:40.040900Z","iopub.execute_input":"2023-08-31T14:05:40.041862Z","iopub.status.idle":"2023-08-31T14:05:40.049730Z","shell.execute_reply.started":"2023-08-31T14:05:40.041825Z","shell.execute_reply":"2023-08-31T14:05:40.048723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Có một số từ bị annotation sai, ta sẽ tiến hành relabel những từ này","metadata":{}},{"cell_type":"code","source":"# annos = [\n#     \"[ command : tăng ] [ device : quạt hút mùi ] lên số [ target number : 3 ] vào [ target number : 23 ] giờ 40 phút\",\n#     \"[ command : tăng ] cho em cái [ device : quạt hút mùi ] lên số [ target number : 3 ] vào lúc [ target number : 23 ] giờ 40 phút nhá\",\n#     \"[ command : tăng ] [ device : quạt hút mùi ] lên [ target number : 3 ] lúc [ target number : 23 ] giờ 40 phút\",\n#     \"em ơi [ command : giảm ] cho anh cái [ device : bóng ] ở [ location : phòng thờ ] xuống [ target number : 77% ] với\",\n#     \"[ command : tăng ] cái [ device : quạt hút mùi ] lên số [ target number : 3 ] lúc [ target number : 23 ] giờ 40 phút nhá\",\n#     \"hãy [ command : tăng ] giúp tôi cái [ device : quạt hút mùi ] lên số [ target number : 3 ] vào lúc [ target number : 23 ] giờ 40 phút nhé\"\n# ]\n# train_stage2.loc[wrong_anno.index, \"sentence_annotation\"] = annos\n# train_stage2.loc[wrong_anno.index]","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:46.743633Z","iopub.execute_input":"2023-08-31T11:42:46.744099Z","iopub.status.idle":"2023-08-31T11:42:46.783859Z","shell.execute_reply.started":"2023-08-31T11:42:46.744057Z","shell.execute_reply":"2023-08-31T11:42:46.782890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tính lại giá trị `tags` sau khi fix annotation","metadata":{}},{"cell_type":"code","source":"# train_stage2[\"tags\"] = train_stage2.parallel_apply(make_tags, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:46.785369Z","iopub.execute_input":"2023-08-31T11:42:46.786348Z","iopub.status.idle":"2023-08-31T11:42:47.700680Z","shell.execute_reply.started":"2023-08-31T11:42:46.786311Z","shell.execute_reply":"2023-08-31T11:42:47.699494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_stage2[train_stage2[\"words\"].apply(len) != train_stage2[\"tags\"].apply(len)]","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:47.702459Z","iopub.execute_input":"2023-08-31T11:42:47.703615Z","iopub.status.idle":"2023-08-31T11:42:47.730269Z","shell.execute_reply.started":"2023-08-31T11:42:47.703572Z","shell.execute_reply":"2023-08-31T11:42:47.729280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"!mkdir data\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(train_stage2, test_size=0.1, random_state=42)\n\nwith open('data/full_stage2.jsonl', 'w', encoding='utf-8') as file:\n    train_stage2.to_json(\"data/full_stage2.jsonl\", lines=True, orient=\"records\", force_ascii=False)\n\nwith open('data/train_stage2.jsonl', 'w', encoding='utf-8') as file:\n    train.to_json(\"data/train_stage2.jsonl\", lines=True, orient=\"records\", force_ascii=False)\n    \nwith open('data/valid_stage2.jsonl', 'w', encoding='utf-8') as file:\n    test.to_json(\"data/valid_stage2.jsonl\", lines=True, orient=\"records\", force_ascii=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:47.731748Z","iopub.execute_input":"2023-08-31T11:42:47.732273Z","iopub.status.idle":"2023-08-31T11:42:49.781929Z","shell.execute_reply.started":"2023-08-31T11:42:47.732236Z","shell.execute_reply":"2023-08-31T11:42:49.780709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slots = train['tags'].explode().unique()\nwith open(\"data/slot\", \"w\") as f:\n    for line in slots:\n        f.write(line + \"\\n\") ","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:49.783664Z","iopub.execute_input":"2023-08-31T11:42:49.784023Z","iopub.status.idle":"2023-08-31T11:42:49.800196Z","shell.execute_reply.started":"2023-08-31T11:42:49.783988Z","shell.execute_reply":"2023-08-31T11:42:49.799061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"intents = train['intent'].unique()\nwith open(\"data/intent\", \"w\") as f:\n    for line in intents:\n        f.write(line + \"\\n\") ","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:49.801949Z","iopub.execute_input":"2023-08-31T11:42:49.802356Z","iopub.status.idle":"2023-08-31T11:42:49.811919Z","shell.execute_reply.started":"2023-08-31T11:42:49.802324Z","shell.execute_reply":"2023-08-31T11:42:49.810933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slots, intents","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:49.813634Z","iopub.execute_input":"2023-08-31T11:42:49.814005Z","iopub.status.idle":"2023-08-31T11:42:49.829594Z","shell.execute_reply.started":"2023-08-31T11:42:49.813972Z","shell.execute_reply":"2023-08-31T11:42:49.828541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"%%writefile config.py\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntotal_epoch = 500\nmax_len = 50\nbatch = 16\nlearning_rate = 0.001\nDROPOUT = 0.2 # 0.2, 0.3, 0.4\n\n\nembedding_size = 300\nlstm_hidden_size = 200\n\n\ntrain_file = 'data/train_stage2.jsonl'\nvalid_file = 'data/valid_stage2.jsonl'\n\nvocab_intent_file = 'data/intent'\nvocab_slot_file = 'data/slot'","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:49.831154Z","iopub.execute_input":"2023-08-31T11:42:49.831513Z","iopub.status.idle":"2023-08-31T11:42:49.840435Z","shell.execute_reply.started":"2023-08-31T11:42:49.831477Z","shell.execute_reply":"2023-08-31T11:42:49.839493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile make_dict.py\nimport pandas as pd\nimport json\nimport config as cf\n\ndef read_file(filepath):\n    lines = []\n    with open(filepath) as f:\n        for line in f.readlines():\n            lines.append(json.loads(line))\n    df = pd.DataFrame(lines)\n    return df\n\ntrain_data = read_file(cf.train_file)\ntrain_data[\"sentence_len\"] = train_data[\"words\"].apply(len)\nvalid_data = read_file(cf.valid_file)\n\n# Xây dựng vocab cho word và tag\nwords = list(train_data['words'].explode().unique())\nslots = list(train_data['tags'].explode().unique())\n\n# Tạo dict word to index, thêm 2 từ đặc biệt là Unknown và Padding\nword2idx = {w : i + 2 for i, w in enumerate(words)}\nword2idx[\"UNK\"] = 1\nword2idx[\"PAD\"] = 0\n\n## Tạo dict slot to index, thêm 1 tag đặc biệt là Padding\n# slot2idx = {t : i + 1 for i, t in enumerate(slots)}\n# slot2idx[\"PAD\"] = 0\n\nslot2idx = {t : i for i, t in enumerate(slots)}\n\n# Tạo 2 dict index to word và index to slot\nidx2word = {i: w for w, i in word2idx.items()}\nidx2slot = {i: w for w, i in slot2idx.items()}\n\n# Tạo intent dict \nidx2intent = {i : val for i, val in enumerate(train_data[\"intent\"].unique())}\nintent2idx = {val : i for i, val in idx2intent.items()}\n\nprint('Number of training samples: ', len(train_data))\nprint('Number of test samples: ', len(valid_data))\nprint('Number of words: ', len(word2idx))\nprint('Number of intent labels: ', len(intent2idx))\nprint('Number of slot labels', len(slot2idx))","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:49.848150Z","iopub.execute_input":"2023-08-31T11:42:49.848422Z","iopub.status.idle":"2023-08-31T11:42:49.856169Z","shell.execute_reply.started":"2023-08-31T11:42:49.848398Z","shell.execute_reply":"2023-08-31T11:42:49.855136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python make_dict.py","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:49.857468Z","iopub.execute_input":"2023-08-31T11:42:49.858462Z","iopub.status.idle":"2023-08-31T11:42:55.741549Z","shell.execute_reply.started":"2023-08-31T11:42:49.858425Z","shell.execute_reply":"2023-08-31T11:42:55.740217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile model.py\nfrom make_dict import word2idx, intent2idx, slot2idx\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom config import device, DROPOUT\nimport config as cfg\n\n\n# Bi-model \nclass slot_enc(nn.Module):\n    def __init__(self, embedding_size, lstm_hidden_size, vocab_size=len(word2idx)):\n        super(slot_enc, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_size).to(device)\n        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=lstm_hidden_size, num_layers=2,\\\n                            bidirectional= True, batch_first=True) #, dropout=DROPOUT)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = F.dropout(x, DROPOUT)       \n        x, _ = self.lstm(x)\n        x = F.dropout(x, DROPOUT)\n        return x \n\n\nclass slot_dec(nn.Module):\n    def __init__(self, lstm_hidden_size, label_size=len(slot2idx)):\n        super(slot_dec, self).__init__()\n        self.lstm = nn.LSTM(input_size=lstm_hidden_size*5, hidden_size=lstm_hidden_size, num_layers=1)\n        self.fc = nn.Linear(lstm_hidden_size, label_size)\n        self.hidden_size = lstm_hidden_size\n\n    def forward(self, x, hi):\n        batch = x.size(0)\n        length = x.size(1)\n        dec_init_out = torch.zeros(batch, 1, self.hidden_size).to(device)\n        hidden_state = (torch.zeros(1, 1, self.hidden_size).to(device), \\\n                        torch.zeros(1, 1, self.hidden_size).to(device))\n        x = torch.cat((x, hi), dim=-1)\n\n        x = x.transpose(1, 0)  # 50 x batch x feature_size\n        x = F.dropout(x, DROPOUT)\n        all_out = []\n        for i in range(length):\n            if i == 0:\n                out, hidden_state = self.lstm(torch.cat((x[i].unsqueeze(1), dec_init_out), dim=-1), hidden_state)\n            else:\n                out, hidden_state = self.lstm(torch.cat((x[i].unsqueeze(1), out), dim=-1), hidden_state)\n            all_out.append(out)\n        output = torch.cat(all_out, dim=1) # 50 x batch x feature_size\n        x = F.dropout(x, DROPOUT)\n        res = self.fc(output)\n        return res \n\n\n\nclass intent_enc(nn.Module):\n    def __init__(self, embedding_size, lstm_hidden_size, vocab_size=len(word2idx)):\n        super(intent_enc, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_size).to(device)\n        # self.embedding.weight.data.uniform_(-1.0, 1.0)\n        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size= lstm_hidden_size, num_layers=2,\\\n                            bidirectional= True, batch_first=True, dropout=DROPOUT)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = F.dropout(x, DROPOUT)\n        x, _ = self.lstm(x)\n        x = F.dropout(x, DROPOUT)\n        return x\n\n\nclass intent_dec(nn.Module):\n    def __init__(self, lstm_hidden_size, label_size=len(intent2idx)):\n        super(intent_dec, self).__init__()\n        self.lstm = nn.LSTM(input_size=lstm_hidden_size*4, hidden_size=lstm_hidden_size, batch_first=True, num_layers=1)#, dropout=DROPOUT)\n        self.fc = nn.Linear(lstm_hidden_size, label_size)\n        \n    def forward(self, x, hs, real_len):\n        batch = x.size()[0]\n        real_len = torch.tensor(real_len).to(device)\n        x = torch.cat((x, hs), dim=-1)\n        x = F.dropout(x, DROPOUT)\n        x, _ = self.lstm(x)\n        x = F.dropout(x, DROPOUT)\n\n        index = torch.arange(batch).long().to(device)\n        state = x[index, real_len-1, :]\n        \n        res = self.fc(state.squeeze())\n        return res\n        \n\n\nclass Intent(nn.Module):\n    def __init__(self):\n        super(Intent, self).__init__()\n        self.enc = intent_enc(cfg.embedding_size, cfg.lstm_hidden_size).to(device)\n        self.dec = intent_dec(cfg.lstm_hidden_size).to(device)\n        self.share_memory = torch.zeros(cfg.batch, cfg.max_len, cfg.lstm_hidden_size * 2).to(device)\n    \n\nclass Slot(nn.Module):\n    def __init__(self):\n        super(Slot, self).__init__()\n        self.enc = slot_enc(cfg.embedding_size, cfg.lstm_hidden_size).to(device)\n        self.dec = slot_dec(cfg.lstm_hidden_size).to(device)\n        self.share_memory = torch.zeros(cfg.batch, cfg.max_len, cfg.lstm_hidden_size * 2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:55.743863Z","iopub.execute_input":"2023-08-31T11:42:55.744279Z","iopub.status.idle":"2023-08-31T11:42:55.756209Z","shell.execute_reply.started":"2023-08-31T11:42:55.744240Z","shell.execute_reply":"2023-08-31T11:42:55.755194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile utils.py\nimport torch\nimport numpy as np\nfrom torch.nn import functional as F\nfrom config import max_len, batch\nfrom make_dict import slot2idx\n\n\ndef make_mask(real_len, max_len=max_len, label_size=len(slot2idx), batch=batch):\n    mask = torch.zeros(batch, max_len, label_size)\n    for index, item in enumerate(real_len):\n        mask[index, :item, :] = 1.0\n    return mask\n\n\ndef masked_log_softmax(vector: torch.Tensor, mask: torch.Tensor, dim: int = -1) -> torch.Tensor:\n    if mask is not None:\n        mask = mask.float()\n        while mask.dim() < vector.dim():\n            mask = mask.unsqueeze(1)\n\n        vector = vector + (mask + 1e-45).log()\n    return torch.nn.functional.log_softmax(vector, dim=dim)\n\n\ndef one_hot(array, Num=len(slot2idx), maxlen=max_len):\n\n    shape = array.size()\n    batch = shape[0]\n    if len(shape) == 1:\n        res = torch.zeros(batch, Num)\n        for i in range(batch):\n            res[i][array[i]] = 1\n    else:\n        res = torch.zeros(batch, maxlen, Num)\n        for i in range(batch):\n            for j in range(maxlen):\n                if array[i, j] == Num:\n                    pass\n                else:\n                    res[i][j][array[i, j]] = 1\n    return res\n\nimport random\n\ndef get_batch(data, batch_size=batch):\n    random.shuffle(data)\n    sindex = 0\n    eindex = batch_size\n    while eindex < len(data):\n\n        sentence = []\n        real_len = []\n        slot_label = []\n        intent_label = []\n         \n        batch = data[sindex:eindex]\n        for m in range(sindex, eindex):\n            sentence.append(data[m][0])\n            real_len.append(data[m][1])\n            slot_label.append(data[m][2])\n            intent_label.append(data[m][3])\n\n        temp = eindex\n        eindex = eindex + batch_size\n        sindex = temp\n\n        yield (sentence, real_len, slot_label, intent_label)\n\ndef get_chunks(labels):\n    chunks = []\n    start_idx,end_idx = 0,0\n    for idx in range(1,len(labels)-1):\n        chunkStart, chunkEnd = False,False\n        if labels[idx-1] not in ('O', '<pad>', '<unk>', '<s>', '</s>', '<STOP>', '<START>'):\n            prevTag, prevType = labels[idx-1][:1], labels[idx-1][2:]\n        else:\n            prevTag, prevType = 'O', 'O'\n        if labels[idx] not in ('O', '<pad>', '<unk>', '<s>', '</s>', '<STOP>', '<START>'):\n            Tag, Type = labels[idx][:1], labels[idx][2:]\n        else:\n            Tag, Type = 'O', 'O'\n        if labels[idx+1] not in ('O', '<pad>', '<unk>', '<s>', '</s>', '<STOP>', '<START>'):\n            nextTag, nextType = labels[idx+1][:1], labels[idx+1][2:]\n        else:\n            nextTag, nextType = 'O', 'O'\n\n        if (Tag == 'B' and prevTag in ('B', 'I', 'O')) or (prevTag, Tag) in [('O', 'I'), ('E', 'E'), ('E', 'I'), ('O', 'E')]:\n            chunkStart = True\n        if Tag != 'O' and prevType != Type:\n            chunkStart = True\n\n        if (Tag in ('B','I') and nextTag in ('B','O')) or (Tag == 'E' and nextTag in ('E', 'I', 'O')):\n            chunkEnd = True\n        if Tag != 'O' and Type != nextType:\n            chunkEnd = True\n\n        if chunkStart:\n            start_idx = idx\n        if chunkEnd:\n            end_idx = idx\n            chunks.append((start_idx,end_idx,Type))\n            start_idx,end_idx = 0,0\n    return chunks","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:55.757964Z","iopub.execute_input":"2023-08-31T11:42:55.758593Z","iopub.status.idle":"2023-08-31T11:42:55.771988Z","shell.execute_reply.started":"2023-08-31T11:42:55.758559Z","shell.execute_reply":"2023-08-31T11:42:55.770822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile make_data.py\nfrom make_dict import train_data, valid_data, word2idx, slot2idx, intent2idx\nfrom config import max_len\nimport numpy as np\nfrom keras_preprocessing.sequence import pad_sequences\n\ndef make_idxdata(data):\n    sentences = data[\"words\"]\n    slots = data[\"tags\"]\n    \n    # Chuyển các câu về dạng vector of index\n    sentence_idx = [[word2idx.get(w, word2idx['UNK']) for w in s] for s in sentences.values]\n\n    # Padding các câu về max_len\n    sentence_idx = pad_sequences(maxlen = max_len, sequences = sentence_idx, padding = \"post\", value = word2idx[\"PAD\"]).tolist()\n\n    # Chuyển các slot về dạng index\n    slot_idx = [[slot2idx[w] for w in s] for s in slots.values]\n\n    # Tiến hành padding về max_len\n    slot_idx = pad_sequences(maxlen = max_len, sequences = slot_idx, padding = \"post\", value = slot2idx[\"O\"]).tolist()\n\n    # Chuyển intent về index\n    intent_idx = [intent2idx[s] for s in data[\"intent\"].values]\n#     print(max_len)\n    return list(zip(sentence_idx, sentences.apply(len).values, slot_idx, intent_idx))\n\ntrain_data = make_idxdata(train_data)\nvalid_data = make_idxdata(valid_data)\n# print(valid_data)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:55.773650Z","iopub.execute_input":"2023-08-31T11:42:55.774310Z","iopub.status.idle":"2023-08-31T11:42:55.786966Z","shell.execute_reply.started":"2023-08-31T11:42:55.774270Z","shell.execute_reply":"2023-08-31T11:42:55.785819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile train.py\nfrom torch import optim\nimport numpy as np\nimport torch\nimport pandas as pd\n\nimport utils\nfrom utils import get_chunks\nfrom config import device\nimport config as cfg\nfrom make_dict import idx2slot\nfrom make_data import train_data, valid_data\nfrom model import *\nfrom collections import Counter\n\nepoch_num = cfg.total_epoch\n\nslot_model = Slot().to(device)\nintent_model = Intent().to(device)\n\nprint(slot_model)\nprint(intent_model)\n\nslot_optimizer = optim.Adam(slot_model.parameters(), lr=cfg.learning_rate)       # optim.Adamax\nintent_optimizer = optim.Adam(intent_model.parameters(), lr=cfg.learning_rate)   # optim.Adamax\n\nbest_correct_num = 0\nbest_epoch = -1\nbest_F1_score = 0.0\nbest_epoch_slot = -1\nbest_epoch_utterance = -1\nbest_utterance = 0\n\nfor epoch in range(epoch_num):\n    slot_loss_history = []\n    intent_loss_history = []\n    for batch_index, data in enumerate(utils.get_batch(train_data)):\n\n        # Preparing data\n        sentence, real_len, slot_label, intent_label = data\n\n        mask = utils.make_mask(real_len).to(device)\n        x = torch.tensor(sentence).to(device)\n        y_slot = torch.tensor(slot_label).to(device)\n        y_slot = utils.one_hot(y_slot).to(device)\n        y_intent = torch.tensor(intent_label).to(device)\n        y_intent = utils.one_hot(y_intent, Num=15).to(device)\n\n        # Calculate compute graph\n        slot_optimizer.zero_grad()\n        intent_optimizer.zero_grad()\n\n        hs = slot_model.enc(x)\n        slot_model.share_memory = hs.clone()\n\n        hi = intent_model.enc(x)\n        intent_model.share_memory = hi.clone()\n\n\n        slot_logits = slot_model.dec(hs, intent_model.share_memory.detach())\n        log_slot_logits = utils.masked_log_softmax(slot_logits, mask, dim=-1)\n        slot_loss = -1.0*torch.sum(y_slot*log_slot_logits)\n        slot_loss_history.append(slot_loss.item())\n        slot_loss.backward()\n        torch.nn.utils.clip_grad_norm_(slot_model.parameters(), 5.0)\n        slot_optimizer.step()\n\n        # Asynchronous training\n        intent_logits = intent_model.dec(hi, slot_model.share_memory.detach(), real_len)\n        log_intent_logits = F.log_softmax(intent_logits, dim=-1)\n        intent_loss = -1.0*torch.sum(y_intent*log_intent_logits)\n        intent_loss_history.append(intent_loss.item())\n        intent_loss.backward()\n        torch.nn.utils.clip_grad_norm_(intent_model.parameters(), 5.0)\n        intent_optimizer.step()\n        \n        # Log\n        if batch_index % 100 == 0 and batch_index > 0:\n            print('Slot loss: {:.4f} \\t Intent loss: {:.4f}'.format(sum(slot_loss_history[-100:])/100.0, \\\n                sum(intent_loss_history[-100:])/100.0))\n\n    # Evaluation \n    total_valid = len(valid_data)\n    correct_num = 0\n    TP, FP, FN = 0, 0, 0\n    utterance_true = 0\n    for batch_index, data_test in enumerate(utils.get_batch(valid_data, batch_size=1)):\n        sentence_test, real_len_test, slot_label_test, intent_label_test = data_test\n        # print(sentence[0].shape, real_len.shape, slot_label.shape)\n        x_test = torch.tensor(sentence_test).to(device)\n\n        mask_test = utils.make_mask(real_len_test, batch=1).to(device)\n        # Slot model generate hs_test and intent model generate hi_test\n        hs_test = slot_model.enc(x_test)\n        hi_test = intent_model.enc(x_test)\n\n        # Slot\n        slot_logits_test = slot_model.dec(hs_test, hi_test)\n        log_slot_logits_test = utils.masked_log_softmax(slot_logits_test, mask_test, dim=-1)\n        slot_pred_test = torch.argmax(log_slot_logits_test, dim=-1)\n        # Intent\n        intent_logits_test = intent_model.dec(hi_test, hs_test, real_len_test)\n        log_intent_logits_test = F.log_softmax(intent_logits_test, dim=-1)\n        res_test = torch.argmax(log_intent_logits_test, dim=-1)\n        \n\n        if res_test.item() == intent_label_test[0]:\n            correct_num += 1\n        if correct_num > best_correct_num:\n            best_correct_num = correct_num\n            best_epoch = epoch\n#             # Save and load the entire model.\n#             torch.save(intent_model, 'model_intent_best.ckpt')\n#             torch.save(slot_model, 'model_slot_best.ckpt')\n    \n        # Calc slot F1 score\n        \n        slot_pred_test = slot_pred_test[0][:real_len_test[0]]\n        slot_label_test = slot_label_test[0][:real_len_test[0]]\n\n        slot_pred_test = [int(item) for item in slot_pred_test]\n        slot_label_test = [int(item) for item in slot_label_test]\n\n        slot_pred_test = [idx2slot[item] for item in slot_pred_test]\n        slot_label_test = [idx2slot[item] for item in slot_label_test]\n\n        pred_chunks = get_chunks(['O'] + slot_pred_test + ['O'])\n        label_chunks = get_chunks(['O'] + slot_label_test + ['O'])\n        for pred_chunk in pred_chunks:\n            if pred_chunk in label_chunks:\n                TP += 1\n            else:\n                FP += 1\n        for label_chunk in label_chunks:\n            if label_chunk not in pred_chunks:\n                FN += 1\n                \n        if (res_test.item() == intent_label_test[0]) and (Counter(pred_chunks) == Counter(label_chunks)):\n            utterance_true += 1\n        if utterance_true > best_utterance:\n            best_utterance = utterance_true\n            best_epoch_utterance = epoch\n            # Save and load the entire model.\n            torch.save(intent_model, 'model_intent_best.ckpt')\n            torch.save(slot_model, 'model_slot_best.ckpt')\n\n    F1_score = 100.0*2*TP/(2*TP+FN+FP)\n    if F1_score > best_F1_score:\n        best_F1_score = F1_score\n        best_epoch_slot = epoch\n        \n    print('*'*20)\n    print('Epoch: [{}/{}], Intent Val Acc: {:.4f} \\t Slot F1 score: {:.4f} \\t Utterance Accuracy: {:.4f}'.format(epoch+1, epoch_num, 100.0*correct_num/total_valid, F1_score, utterance_true/total_valid))\n    print('*'*20)\n    \n    print('Best Intent Acc: {:.4f} at Epoch: [{}]'.format(100.0*best_correct_num/total_valid, best_epoch+1))\n    print('Best F1 score: {:.4f} at Epoch: [{}]'.format(best_F1_score, best_epoch_slot+1))\n    print('Best Utterance Acc: {:.4f} at Epoch: [{}]'.format(best_utterance/total_valid, best_epoch_utterance+1))\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:59.644782Z","iopub.execute_input":"2023-08-31T11:42:59.645168Z","iopub.status.idle":"2023-08-31T11:42:59.659413Z","shell.execute_reply.started":"2023-08-31T11:42:59.645132Z","shell.execute_reply":"2023-08-31T11:42:59.657686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python train.py","metadata":{"execution":{"iopub.status.busy":"2023-08-31T11:42:59.661481Z","iopub.execute_input":"2023-08-31T11:42:59.662031Z","iopub.status.idle":"2023-08-31T13:38:14.555310Z","shell.execute_reply.started":"2023-08-31T11:42:59.661996Z","shell.execute_reply":"2023-08-31T13:38:14.554089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load model weights","metadata":{}},{"cell_type":"code","source":"import torch\nslot_model = torch.load(\"/kaggle/working/model_slot_best.ckpt\")\nintent_model = torch.load(\"/kaggle/working/model_intent_best.ckpt\")","metadata":{"execution":{"iopub.status.busy":"2023-08-31T13:38:14.558653Z","iopub.execute_input":"2023-08-31T13:38:14.559009Z","iopub.status.idle":"2023-08-31T13:38:18.012581Z","shell.execute_reply.started":"2023-08-31T13:38:14.558979Z","shell.execute_reply":"2023-08-31T13:38:18.011506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from make_data import valid_data","metadata":{"execution":{"iopub.status.busy":"2023-08-31T13:38:18.015295Z","iopub.execute_input":"2023-08-31T13:38:18.016200Z","iopub.status.idle":"2023-08-31T13:38:18.173865Z","shell.execute_reply.started":"2023-08-31T13:38:18.016160Z","shell.execute_reply":"2023-08-31T13:38:18.172843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import utils\nimport torch.nn.functional as F \nfrom make_dict import idx2intent, idx2slot, idx2word\ndevice = \"cuda\"\n\nfor batch_index, data_test in enumerate(utils.get_batch(valid_data[-10:], batch_size=1)):\n    sentence_test, real_len_test, slot_label_test, intent_label_test = data_test\n    x_test = torch.tensor(sentence_test).to(device)\n    mask_test = utils.make_mask(real_len_test, batch=1).to(device)\n#     print(x_test)\n    # Slot model generate hs_test and intent model generate hi_test\n    hs_test = slot_model.enc(x_test)\n    hi_test = intent_model.enc(x_test)\n\n    # Slot\n    slot_logits_test = slot_model.dec(hs_test, hi_test)\n    log_slot_logits_test = utils.masked_log_softmax(slot_logits_test, mask_test, dim=-1)\n    slot_pred_test = torch.argmax(log_slot_logits_test, dim=-1)\n    # Intent\n    intent_logits_test = intent_model.dec(hi_test, hs_test, real_len_test)\n    log_intent_logits_test = F.log_softmax(intent_logits_test, dim=-1)\n    res_test = torch.argmax(log_intent_logits_test, dim=-1)\n    \n    print(\"Itent: \")\n    print(\"Predict: \", idx2intent[res_test.item()])\n    print(\"Label: \", idx2intent[intent_label_test[0]])\n    \n    print(\"Slot: \")\n    slot_pred_test = slot_pred_test[0][:real_len_test[0]]\n    slot_label_test = slot_label_test[0][:real_len_test[0]]\n\n    slot_pred_test = [int(item) for item in slot_pred_test]\n    slot_label_test = [int(item) for item in slot_label_test]\n\n    slot_pred_test = [idx2slot[item] for item in slot_pred_test]\n    slot_label_test = [idx2slot[item] for item in slot_label_test]\n    print(\"Predict: \" , slot_pred_test)\n    print(\"Labels: \", slot_label_test)\n    print(\"Sentence: \", [[idx2word[w] for w in item] for item in sentence_test])\n    print(\"=================\")","metadata":{"execution":{"iopub.status.busy":"2023-08-31T13:38:18.175496Z","iopub.execute_input":"2023-08-31T13:38:18.175818Z","iopub.status.idle":"2023-08-31T13:38:19.128127Z","shell.execute_reply.started":"2023-08-31T13:38:18.175786Z","shell.execute_reply":"2023-08-31T13:38:19.127174Z"},"trusted":true},"execution_count":null,"outputs":[]}]}